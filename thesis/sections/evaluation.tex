\chapter{Evaluation (probably rename as well)} \label{chap:evaluation}

The key difference between the results here presented and the ones in the previous section would be that in the previous chapter we would only run micro-benchmarks to validate that our implementation is sound.

In this chapter however my aim would be to both run some macro-benchmarks in the sense that we evaluate for instance: iterative + remote + diskless migration, or an assesment of a real-application using TCP connections (e.g. our database or a VPN server) and measure interesting things.

\section{Macro-Benchmarks}

Run some macrobenchmarks we can think of namely:

\subsection{Iterative, Remote, Diskless Migration}

Goal here would be to have the diagram we went over once, in which we plot over time the amount of memory we send.

\subsection{Checkpointing TCP Connections}

Maybe measure throughput and downtime

\subsection{More Large-scope Experiments we can think of}

\section{Performance of \projName}

In this section I would aim to measure the performance of our distributed checkpointing in the form it ends up happening and from a number of different perspectives.

\cs{Here it is very important to try and find some systems we could compare against with.}
Some that come of the top of my mind are:
\begin{itemize}
    \item Raw CRIU: given that we also have PoCs for the different features using only CRIU, maybe we could compare using that.
    \item VM system: I am unaware if there is a way to checkpoint a network of VMs and then restart them.
    
    \jg{At least you should be able to compare with a VM system for the migration of a single VM}.
\end{itemize}

\subsection{Comparison Against WHAT?}
