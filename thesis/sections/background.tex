\chapter{Background Concepts} \label{chap:background}

The main goal of this project is to implement live migration of running containers.
It builds on the concepts of containers, checkpointing, and its implementation in the \criu project.
This chapter provides a detailed introduction to these concepts as they are necessary to understand the contributions we present later on.

\section{Containers} \label{sec:containers}

A Linux container is a set of isolated processes with a limited view of their environment.
They build on traditional concepts of virtualization, and provide an alternative to virtual machines (VM).
A container is usually faster, lighter, and more flexible than a VM.
As a consequence they are becoming the technology of choice in multi-tenant settings, such as data centers.

\subsection{An Introduction to Virtualization}

Virtualization is a recurrent technique in systems design in computer science which aims to provide processes the illusion that they interact with a defined interface, hiding the real implementation behind.
Some of the most relevant features facilitated by virtualization are: process isolation from other processes and the underlying system, fine-grained dynamic resource provisioning, multiple virtually dedicated subsystems on the same physical instance, among others.
We classify virtualization techniques according to the type of interface being virtualized.

\textbf{Emulation.}
Emulators allow applications written for a certain computer architecture to run on a different one.
They do so by translating (\textit{i.e.} virtualizing) the Instruction Set Architecture (ISA).
An example of such a system is \textsc{Qemu} (\url{https://www.qemu.org/}).

\textbf{Hardware Virtualization.}
Hardware virtualization interfaces a complete system which enables to run a fully-featured operating system within a different one.
It has traditionally been one of the most user-friendly virtualization tools in the form of \emph{virtual machines} such as the Linux-Kernel Virtual Machine \textsc{KVM} (\url{https://www.linux-kvm.org/page/Main_Page}, \textsc{VMWare Workstation} (\url{https://www.vmware.com/}) or Oracle's \textsc{VirtualBox} (\url{https://www.virtualbox.org/}).
We differentiate between full virtualization and paravirtualization.
The former adds an hypervisor or virtual machine monitor (VMM) which creates the illusion of multiple virtual machines, which are multiplexed across the physical resources, and allow to run an \emph{unmodified} guest OS.
The latter modifies the guest OS' source code and replaces sensitive calls with \emph{hypercalls}, which are direct calls to the hypervisor.

\textbf{OS-level Virtualization.}
Operating System-level virtualization allows for multiple isolated user-space instances, called \textbf{containers} which share a single operating system.
In comparison to traditional virtual machines, containers add little overhead, require minimal startup, and have a low resource requirement, these factors make them highly scalable.
Containers have experienced an exponential increase in usage, specially with the advent of open-source highly-available container engines such as \textsc{Docker} (\url{https://www.docker.com/}), Linux Containers \textsc{LXC} (\url{https://linuxcontainers.org/}), \textsc{Podman} (\url{https://podman.io/}, among others.
Given that the goal of this project is to perform efficient live migration of running containers, the following section provides further technical details on containerization.

\subsection{Working Principles of Containers}

As previously introduced, a Linux container is a set of processes that are isolated from the rest of the machine.
To achieve this isolation, they rely on two kernel features: control groups and namespaces~\cite{namespaces-manual}.

\subsubsection*{Namespaces}

As greatly phrased by Michael Kerrisk in his series of articles on namespaces~\cite{Kerrisk2013}, the purpose of namespaces is to wrap a global system resource and abstract it in a way that each process within the namespace thinks it has its own isolated instance of such resource.
As of Kernel 5.6, there are eight different types of namespaces, which we present together with a brief description in Table~\ref{table:namespaces}.
\begin{table}[h!]
    \centering
    \rowcolors{1}{gray!20}{}
    \begin{tabular}{p{2cm}p{13cm}}
        \hline
        \textbf{Kind} & \textbf{Description} \\[3pt]
        \hline \hline
        \texttt{mnt} & \textbf{Mount namespaces} provide isolation of the list of mount points seen by the process in each namespace instance. It allow processes to have their own root file system and mount and unmount file systems without affecting the rest of the system. \\[3pt]
        \texttt{pid} & The \textbf{process ID} isolates the PID number space. This means that two processes in different PID namespaces can have the same identifier. It is very useful in container migration as it allows to restore the processes with the same PID they were dumped with regardless of whether that ID might be taken in the target machine or not. \\[3pt]
        \texttt{net} & \textbf{Network namespaces} provide isolation of the whole network stack. In particular network devices, interfaces, routing tables, iptables rules, and sockets. \\[3pt]
        \texttt{ipc} & The \textbf{Interprocess Communication namespace} provides isolation for POSIX semaphore queues, semaphore sets and shared memory segments.\\[3pt]
        \texttt{uts} & The \textbf{UNIX Time Sharing namespace} allows processes to set a hostname or domain name for that particular namespace without affecting the rest of the system. \\[3pt]
        \texttt{user} & \textbf{User namespaces} isolate security-related identifiers such as user and group identifiers (UID, GID) and capabilities. This allows for a process to have privileges within a certain namespace but not outside its scope. \\[3pt]
        \texttt{cgroup} & The \textbf{Control Group namespace} virtualizes the contents of \texttt{/proc/self/cgroup}. As a consequence each different namespace has a different \texttt{cgroup} root directory.\\[3pt]
        \texttt{time} & The \textbf{time namespace} has been the latest addition to the group. Included in Kernel 5.6, it allows different namespaces to have different offsets to the system monotonic and boot-time clock.\\[3pt]
        \hline
    \end{tabular}
    \caption[List of the different namespaces supported in Kernel 5.6.]{List of the different namespaces supported in Kernel 5.6. and a brief description of the isolation they provide.\label{table:namespaces}}
\end{table}
In order to create a new namespace of a given type, we can follow two approaches.
With the \texttt{clone} system call, we can create a new child process, in a similar way to \texttt{fork} but with higher control of what pieces of execution context are inherited~\cite{clone-manual}.
More specifically, with the \texttt{unshare} system call we can specifically unshare a namespace from parent process~\cite{unshare-manual}.
To join an existing namespace, we can use the \texttt{setns} syscall, which, given a file descriptor referring to a namespace, it links the calling process to it~\cite{setns-manual}.
These operations require the \texttt{CAP\_SYS\_ADMIN} capability.
In Listings~\ref{code:namespaces-unshare} and~\ref{code:namespaces-setns} we include examples of usage of \texttt{unshare} and \texttt{setns} respectively.
\begin{figure}[h!]
    \begin{minipage}{.45\textwidth}
        \begin{lstlisting}[language=C,caption={Snippet to unshare the calling thread from a namespace using \texttt{unshare}.\label{code:namespaces-unshare}}]
#define _GNU_SOURCE
#include <errno.h>
#include <sched.h>
#include <stdio.h>

// Unshare from parent namespace
int main(int argc, char *argv[])
{
    // Specify the required flags
    // (bit-wise or)
    flags = CLONE_NEWNET || CLONE_NEWPID;

    // Dettach from parent namespace
    if (unshare(flags) == -1)
    {
        perror("unshare failed");
        exit(EXIT_FAILURE);        
    }

    return 0;
}
\end{lstlisting}
    \end{minipage} \hfill
    \begin{minipage}{.45\textwidth}
        \begin{lstlisting}[language=C,caption={Snippet to attach to an existing network namespace.\label{code:namespaces-setns}}]
#define _GNU_SOURCE
#include <errno.h>
#include <sched.h>
#include <stdio.h>

// Attach calling process to an existing
// network namespace.
int main(int argc, char *argv[])
{
    // Get namespace FD
    fd = open("/proc/330/ns/net", O_RDONLY);

    // Join the namespace
    if (setns(fd, CLONE_NEWNET) == -1)
    {
        perror("setns failed");
        exit(EXIT_FAILURE);        
    }

    return 0;
}
\end{lstlisting}
    \end{minipage}
\end{figure}

\subsubsection*{Control Groups}

Control groups (\texttt{cgroups}) are a resource management kernel feature that allows handling of processes in hierarchical groups.
This way, fine-grained resource metering and limiting can be applied on a per-group basis.
Typical resources monitored using this technique are memory, CPU usage, I/O network, among others.

These constraints are enforced through the usage of kernel subsystems.
Each different subsystem, mapped to one of the resources to manage, has an independent hierarchy.
Each process then belongs to exactly one group per subsystem.
For instance, the memory cgroup keeps track of the pages used and imposes different limits for physical, kernel, and total memory.

\subsection*{Container Terminology}

With the rapid increase in popularity, a wide-range of terminology has also been introduced in the container ecosystem.
These terms are commonly misused~\cite{McCarty2018} and, even though they don't cover the technical principles, are useful to differentiate the services different tools offer.

A \emph{container} is the (set of) isolated Linux process.
It is a running instance of a \emph{container image}, a (set of) files that are used locally as a mount point.
To enhance portability and vendor interoperability, images are stored using a standardized format by the Open Container Initiative (OCI, \url{https://opencontainers.org/}), an open governance structure for container-related standards.
The \emph{container engine} turns the image into a running container and acts as the interaction point with the user.
However, engines don't tend to actually instantiate the containers themselves, and rather rely on a \textit{container runtime}.
The runtime is the lower level component that interacts with the kernel, its specification~\cite{container-runtime-specification} is also maintained and developed by the OCI.
\runc is its reference implementation, and our tool of choice to implement live migration.
We have chosen to skip the engine layer and interact with the runtime as support for advanced \criu features is lacking in higher-level tools (more details on \criu are presented in \S\ref{sec:bg-criu}).

\subsubsection*{Different Container Engines and Runtimes}

There are a variety of container engines available.
\texttt{Docker} was the company that started exploiting containers commercially.
It is most likely the best-known tool and the main responsible for the rapid adoption of the technology.
However, there are several alternatives.

Given its pivotal importance, we differentiate between container engines that rely on \runc as their runtime of choice, and those which don't.

\textbf{\runc-based Container Engines.}

Other than \texttt{Docker}, \texttt{cri-O} (\url{https://cri-o.io/}) is a container engine focused on the integration of lightweight containers with the Kubernetes orchestrator.
\texttt{Podman} (\url{https://podman.io/getting-started/}) is another alternative to \texttt{Docker}.
Its main distinguishing factor is that it is a \textit{daemonless} container engine.
Lastly, \texttt{rkt} (\url{https://github.com/rkt/rkt}) is a project with pluggability, interoperability, and customization in mind.
Unfortunately, it has reached end-of-life and is not currently maintained.

\textbf{Non-\runc Container Engines.}

\texttt{Katacontainers} (\url{https://katacontainers.io/}) is a container runtime that runs on lightweight virtual machines.
This is, instead of relying on standard container isolation techniques, they use VM-native isolation (hardware-backed) to provide further security guarantees.
In particular, each container is hypervisor-isolated, meaning two different containers have different kernels.
\texttt{crun} (\url{https://github.com/containers/crun}) is a re-implementation of \runc in C.
Its main focus is on performance and reduced memory footprint.
In both cases \texttt{crun} surpasses \runc.
\texttt{railcar} (\url{https://github.com/oracle/railcar}) is another runtime implementation born from the idea that \textsc{Go} might not be the best programming language to implement a container runtime.
With memory safety in mind, Oracle, the company behind \texttt{railcar}, decided to implement an OCI-compliant runtime in \textsc{Rust}.
Unfortunately, the project is nowadays archived.

\subsubsection*{\runc: the reference runtime implementation}

Originally developed at Docker, \runc is a lightweight container runtime aimed to provide low-level interaction with containers.
In 2015~\cite{introducing-runc}, Docker open-sourced the component and transferred ownership to the Open Container Initiative (OCI), who has since then lead the project in a fashion similar to that of the Linux Foundation.
Since then, several container engines such as \textsc{Podman} (\url{https://podman.io/}) and \textsc{CRI-O} (\url{https://cri-o.io/}) have made \runc their runtime of choice.

The OCI has since then released specifications for container runtimes, engines, images, and image distribution.
\runc is nowadays an OCI-compliant container runtime (it is, in fact, the reference implementation).

Users are encouraged to interact with containers through container engines, but \runc itself provides an interface to create, run, and manage containers natively.
Integration with \criu has to be done on a per-project basis, and \runc has the most advanced and stable integration.
Therefore, we decided to use it to manage our containers.

Running a container with \runc is slightly different than doing it in, let's say, Docker, as the user's interaction with the underlying system is more direct.
In particular, in \runc there is no notion of \textit{images}.
To run a container, a user must provide a specification file (\texttt{config.json}) and a root file system in a directory (\texttt{rootfs}).
Through the specification file several low-level options such as namespaces, control groups, and capabilities can be configured.
The pair \texttt{config.json} and \texttt{rootfs} is referred to as OCI bundle.

\section{Checkpointing} \label{sec:bg-cp}

Imagine a long-running job in a cluster or in the cloud.
Several hours into execution, the job unreasonably fails.
Even in bug-free software, programs may crash from time to time due to, for instance, hardware failures.
This happens even more in multi-tenant environments where different users are sharing the same physical resources~\cite{Barker2014}.

Losing hours worth of computation is not only a loss of time for developers and scientists, but also a loss of money.
A possible solution would be programatically saving data every certain time, or every certain number of iterations.
This approach requires additional work by the developer, who has to implement not only the saving procedure, but the resuming one, in the event the application needs to be started from an intermediate state.
Alternatively, highly parallel workloads could run processes separately on different chunks of data, and aggregate results afterwards.
However, these solutions are \textit{ad-hoc}, error-prone, and most importantly require additional work from the developer.

\subsection{Checkpoint/Restore}

Checkpointing refers to the ability of storing the state of a computation such that it can be continued later at that same state without covering the preceding ones.
The saved state is called a \textit{checkpoint} and the resumed process a \textit{restart} or \textit{restore}~\cite{Schulz2011}.
It provides systems with additional fault-tolerance and fast rollback times.
C/R tools snapshot an application's state regardless of the software running and without requiring, in general, any additional work from the application developer.
Even though they originated in the High Performance Computing environment, these tools are also useful for debugging, skipping long initialization times, and, as in this work, live process migration.

During checkpoint, and in order to save the process' state, all the essential information such as the program's memory, file descriptors, sockets and pipes, among others are dumped.
In the distributed scenario, additional logic is required to coordinate the checkpoint across all processes~\cite{Raynal2013}.

Checkpoint-Restore became popular in the setting of virtual machines~\cite{Clark2005}, but had already been thoroughly studied in the context of rollback and recovery strategies~\cite{Elnozahy2002}.
VM checkpointing is easier when compared to arbitrary process checkpointing as VMs are already isolated.
Nowadays, and other than \criu which is our tool of choice, there are several mature C/R projects that checkpoint virtually any running process transparently to the user.
A comparison among some of them is presented later in \S\ref{sec:bg-criu}.

\subsection{Live Migration}

A prominent application of Checkpoint/Restore is live migration.
Live migration allows moving a running process from a physical host to another with negligible downtime and transparently to the end user.
It is clear that live migration is a desirable feature for cloud tenants as it drastically increases their load-balancing capabilities with minimal impact to the perceived quality of service.

A naive approach would checkpoint the process in one host, transfer the checkpoint dump through the network, and restore it later on the other host.
Unfortunately, this approach incurs in very high downtimes.
Other more refined and more popular mechanisms minimize this downtime, we highlight \textit{pre-copy} and \textit{post-copy} migration.

\textbf{Pre-Copy Migration.}

Pre-copy migration, or sometimes called iterative migration, is a live migration technique that transfers most of the checkpoint information previously to stopping the running process, and only stops it once the information to transfer to the other end is minimal, achieving a very low downtime~\cite{Bradford2007}.
As memory pages tend to be the largest resource to dump and transfer, most pre-copy implementations iteratively transfer the memory that has changed between subsequent iterations. 
Additionally, and depending on whether the different nodes share a common file system or not, files are also incrementally dumped.
When the information to transfer is lower than a specified threshold, the application is stopped in one end, the remaining bits sent over the wire, and resumed in the other one.

\textbf{Post-Copy Migration.}

Post-copy migration follows a radically different approach.
Initially, it transfers the minimal information for the process to be able to resume on the destination host~\cite{Shirbam2012}.
Then, page faults in the new host are resolved over the network, sending a request for a page that was not sent in the initial batch.
This approach tends to be faster (lower total migration time) than pre-copy, but incurs in service degradation as page faults become extremely costly.
In the literature, post-copy migration is also referred to as \textbf{lazy migration}.

\subsection{Distributed Checkpointing}

A less explored area is that of checkpointing a distributed application as a whole.
This is, given a process running in different physical hosts, how to coordinate the checkpoint in order to get a consistent execution state.
Reaching consensus among a set of distributed processes is a well-studied topic in the distributed systems field, and an existing algorithm for distributed snapshotting was presented in 1985 by Chandy and Lamport~\cite{Lamport1985}.
Since then, more and more algorithms have been presented~\cite{Raynal2013,Kshemkalyani2008} optimizing certain aspects of the process.
However, transparent distributed C/R is yet to be implemented in a software tool.
The work here presented aims to be a step in this direction by facilitating live migration of containers with established TCP connections and external namespaces (external in the sense that they are not created by the migrated process).

\section{\criu: Checkpoint Restore in Userspace} \label{sec:bg-criu}

Checkpoint/Restore in Userspace (\criu) is an open-source C/R tool~\cite{criu-main-page}.
Introduced in 2011, its distinctive feature is that it is mainly implemented in user space, rather than in the kernel, by using existing interfaces~\cite{Reber2016}.
One of the most important interface is \texttt{ptrace}~\cite{ptrace-manpage}, as \criu relies on it for seizing the target process.
For other interfaces, several patches have been pushed to the mainline kernel by \criu developers~\cite{criu-kernel-patches}.
The project is currently under active development~\cite{criu-github}, and its main focus is to support the checkpoint and migration of containers.

\subsection{A Technical Overview on \criu}

The main goal of \criu is to perform a snapshot of the current process' tree state to a set of image files, so that it can be later restored at that exact point in time, without reproducing the steps that led to it.

\subsubsection*{Checkpoint}

The checkpointing process starts with the process identifier (PID) of a process group leader provided by the user through the command line using the \texttt{--tree} option~\cite{criu-checkpoint}.
However, before it can actually start, we need to ensure that the process does not change its state during checkpoint.
This includes: opening file descriptors, changing sessions, or even producing new child processes~\cite{criu-freeze}.
To achieve this transparently, instead of sending a stop signal (which could affect the process' state) \criu freezes tasks using \texttt{ptrace}'s \texttt{PTRACE\_SEIZE} command~\cite{ptrace-manpage}.
In order to find all active tasks descendant of the parent PID, the \texttt{\$pid} dumper iterates through each \texttt{/proc/\$pid/task/} entry, recursively gathering threads and their children from \texttt{/proc/\$pid/task/\$tid/children}.

Once all tasks are frozen, \criu collects all the information it can about the task's resources.
File descriptors and registers are dumped through a \texttt{ptrace} interface and are parsed from \texttt{/proc/\$pid/fd} and \texttt{/proc/\$pid/stat} respectively.
In order to dump the contents of memory and credentials, a novel technique is introduced, the \textbf{parasite code}.

The parasite code is a binary blob built as a position independent executable (PIE) for execution inside another process' address space.
Its purpose is to execute \criu calls from within the dumpee's task address space~\cite{criu-parasite-code}.
To achieve this goal, \criu must:
\begin{enumerate}
    \item Move the task into seized state calling \texttt{ptrace(PTRACE\_SEIZE, ...)}. Note that the task is stopped without it noticing, hence not altering its state.
    \item Inject an \texttt{mmap} syscall in the current stack's instruction pointer, and allocate memory for the whole code blob. At this stage, space for exchanging parameters and results is also allocated within the dumpee's process address space. \criu is now ready to run parasite service routines.
    \item The external dumping process retrieves information about the dumpee's address space through the parasite code either through \emph{trap} mode (one command at a time) or \emph{daemon} mode (in which the parasite behaves as a UNIX socket).
    \item With information about used memory areas and important flags read from \texttt{/proc/\$pid/smaps/} and \texttt{/proc/\$pid/pagemap}, the parasite code transfers the actual content outside through a set of pipes, which in turn gets translated into image files.
\end{enumerate}
Lastly, the target process is cured from the parasite by closing it, unmapping its allocated memory area, and reverting to the original frozen state.

\subsubsection*{Restore}

During the restore process, \criu morphs into the to-be-restored task.
Since we checkpoint process trees rather than single processes, \criu must \texttt{fork} itself several times to recreate the original PID tree.
In particular, and in order to be completely transparent, \criu requires that the restored tasks have the same PID they had before dump.
To achieve this goal, older versions of \criu had to perform very time-sensitive and race condition-prone PID handling, what was referred to as the PID dance~\cite{Reber2019,criu-pid-dance}.
Starting with kernel 5.3 and the new \texttt{clone3()} system call, it becomes now possible to clone a process and specify the desired PID for it~\cite{kernel-clone3}.

Then \criu restores all basic task resources such as file descriptors, namespaces, maps, ...
The only resources that are \emph{not} restored at this stage are, most notably, memory mappings.
In order to restore memory areas, and since the morphing is done \textit{in-place}, before exiting \criu would have to \texttt{unmap} itself and map the application code.
To overcome this issue, a similar approach to the parasite code one is followed, the \textbf{restorer blob}.
The restorer blob is a piece of PIE code, to which \criu transfers control to unmap itself and map the appropriate code and memory areas for the process to restore successfully.

\subsubsection*{Live Migration with \criu}

As \criu operates by design on a single system, support for live migration requires further user interaction~\cite{criu-live-migration}.
In particular, it is up to the user to ensure that the dump files are on the remote host upon restore.
Furthermore, IP addresses used by the application in the original host, must also be available in the new one.
With that said, \criu developers implemented \textsc{P.Haul}~\cite{criu-phaul}, a library specially targeted for live migration using \criu.
At the time of the writing, the project is currently inactive.

The most natural way to manually perform a live migration is to use the iterative approach as we will cover later.
However, support for lazy migration and a page server is also available~\cite{criu-lazy-migration}.
A major drawback with iterative migration is that, as explained before, \criu freezes the process while the snapshot takes place.
As a consequence, recurrent snapshots of a memory intensive application might cause it to freeze during long periods of time.
Lastly, and in order to prevent file duplication, it is encouraged to use a technique called diskless migration~\cite{criu-diskless} - which we will cover in detail in \S\ref{chap:system}.

\subsection{Comparison with Other C/R Tools}

The main differences between C/R tools are the way they interact with the kernel and the type of applications they target.
\criu is implemented completely in userspace, and as a consequence relies heavily on existing kernel interfaces, otherwise execution fails.
Additionally, \criu's target application are containers.

Other open-source tools that implement C/R are \texttt{DMTC}~\cite{dmtcp}, and \texttt{BLCR}~\cite{blcr}.
They both focus on high performance computing, what motivates some of their design choices.

\textbf{DMTCP.}

Distributed Multi-Threaded Checkpointing (DMTCP) is an active project lead by Prof. Cooperman at Northeastern University that implements C/R on a library level.
This means that if a user wants to checkpoint its application, this must be dynamically linked from the very beginning and executed with custom wrappers (which decreases transparency).
DMTCP intercepts all system calls instead of assuming existing kernel interfaces, as \criu does, and is, as a consequence, more robust and reliable.
It is very popular in HPC environments and is present in a variety of publications~\cite{Ansel2009,Garg2019}.

\textbf{BLCR}

Berkeley Lab Checkpoint/Restart (BLCR) is a system-level checkpointing tool aimed also at High Performance Computing jobs.
It requires loading an additional kernel module and is currently not maintained (last supported kernel version is $3.7$).

A detailed table comparing the software here presented, and some other solutions, is maintained by the CRIU foundation~\cite{criu-comparison}.
